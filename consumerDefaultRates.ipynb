{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f05d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# call functions and config modules \n",
    "exec(open(\"./functions/functions.py\").read())\n",
    "exec(open(\"./config/config.py\").read())\n",
    "\n",
    "# set style for notebook \n",
    "_set_css_style('./www/custom.css') \n",
    "\n",
    "# toggle code on/off button \n",
    "HTML('''<script>\n",
    "code_show=true; \n",
    "function code_toggle() {\n",
    " if (code_show){\n",
    " $('div.input').hide();\n",
    " } else {\n",
    " $('div.input').show();\n",
    " }\n",
    " code_show = !code_show\n",
    "} \n",
    "$( document ).ready(code_toggle);\n",
    "</script>\n",
    "<form action=\"javascript:code_toggle()\"><input type=\"submit\" value=\"Click here to toggle on/off the raw code.\"></form>''')\n",
    "\n",
    "\n",
    "# call data manipulation and plotting libraries \n",
    "import json\n",
    "import pandas as pd \n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "import warnings\n",
    "import shap\n",
    "import random\n",
    "import plotly.graph_objects as go\n",
    "from IPython.display import HTML\n",
    "import plotly.io as pio; pio.renderers.default='notebook'\n",
    "import plotly.offline as py\n",
    "py.init_notebook_mode(connected=True)\n",
    "from plotly.offline import init_notebook_mode, iplot\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as offline\n",
    "from datetime import date, time, datetime, timedelta\n",
    "from plydata import define, if_else\n",
    "from numpy import mean, absolute\n",
    "\n",
    "# import librariers for machine learning \n",
    "from statsmodels.formula.api import ols\n",
    "import pycaret \n",
    "import sklearn \n",
    "import featuretools as ft\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from sklearn.metrics import mean_squared_error \n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score \n",
    "from pylab import rcParams\n",
    "from sklearn import metrics \n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from pycaret.classification import *\n",
    "\n",
    "# turn off notebook warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# turn off scientific notation \n",
    "pd.options.display.float_format = '{:.2f}'.format\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b01c107",
   "metadata": {},
   "source": [
    "# Data overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2282c3c",
   "metadata": {},
   "source": [
    "Many people struggle to get loans due to insufficient or non-existent credit histories. And, unfortunately, this is often the most marginalised part of the population. The data set used in this analysis includes consumer default instances, the `target` variable. A default is defined to be the customer that has late payment more than 90 days on a given load or failed to repay the loan. \n",
    "\n",
    "The borrower characteristics include quantative metrics such as income, credit amount, value of good purchased, days employed etc. It also include qualatative/catergorical meausures such as gender, education, home ownership, mobile ownership, number of children etc. \n",
    "\n",
    "This analysis does not take into account financial inclusion for the unbanked population. In fronteir markets, its important to make sure that underserved populations has a positive loan experience and given everyone an fair assessment. This data does not take into account alternative data sources to predict customers repayment abilities.\n",
    "\n",
    "Alternative data sources that could be incorporated into the model includes;\n",
    "\n",
    "* Macro-economic variables, such as inflation, GDP, unemployment, local FX rates, tax etc. This would required a timestamp to identify macro-economic conditions at the time of default. \n",
    "* Recession/pandemic indicators, e.g. extreme macro-economic climates impacting the borrowers ability to repay their loans. \n",
    "* Non-financial assets, such as land, livestock, agriculture machinary atc. \n",
    "* Health related measures or history. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64549154",
   "metadata": {},
   "source": [
    "# Import and interrogate data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24324b04",
   "metadata": {},
   "source": [
    "The feature set includes borrower measures of credit usage, income, annuities, value of good purchases as well as qualative features such as education type, home ownership, birthday, gender etc. The response variable is a borrow DEFAULT (`TARGET = 1`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c3bf6",
   "metadata": {},
   "source": [
    "The **target** variable takes the value **1** if someone experiences payment difficulties or fails to repay a loan. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbd51e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in the credit risk data set\n",
    "credit_risk= pd.read_csv(\"./data/credit_risk_data.csv\")\n",
    "\n",
    "# reset index to ID \n",
    "credit_risk = credit_risk.set_index('SK_ID_CURR')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d4bbb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first look at the structur of the data \n",
    "print('Data types in credit risk dataset as follows:' '\\n')\n",
    "credit_risk.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "655a389e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explore data properties \n",
    "credit_risk.describe(percentiles=[.001, .1, .9, .99])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8cbe87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Null values in dataset are as follows:' '\\n')\n",
    "null = credit_risk.isnull().sum()\n",
    "print(null[null>0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb7c6d0",
   "metadata": {},
   "source": [
    "There are some missing values that need to be imputed or removed from the data, namely 'annuity ammount', 'occupation type' and 'amount good price'. There are also columns that dont contain useful information, such as ID. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b7bd913",
   "metadata": {},
   "outputs": [],
   "source": [
    "colnames = list(credit_risk.columns.drop('TARGET')) # create list of predictor names\n",
    "categorical_names = credit_risk.select_dtypes(include=[object, np.int64]).columns # create list of categorical names\n",
    "continuous_names = credit_risk.select_dtypes(exclude=[object, np.int64]).columns # create list of continuous names\n",
    "\n",
    "print(\"The categorical/discrete features include: \")\n",
    "print(\", \".join(list(map(str,categorical_names))))\n",
    "\n",
    "print(\"\\nThe continuous features include: \")\n",
    "print(\", \".join(list(map(str,continuous_names))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e6c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation matrix of feature set \n",
    "corr = credit_risk[[*continuous_names]].corr()\n",
    "\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f8a733",
   "metadata": {},
   "source": [
    "The above chart shows that the continuous variables are relatively correlated, e.g. a high credit amount correlates with a high good price (correlation = ~0.9). This can be a problem in classification context; however less so in a machine learning context. The problem is that in practice, you need to explain the system’s behaviour, especially if it makes decisions. ML explainability is important so that intelligent technologies don’t inherit societal biases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee403a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_data = credit_risk[['TARGET', *continuous_names]]\n",
    "plot_data = plot_data.melt(id_vars=\"TARGET\", \n",
    "                           var_name=\"name\", \n",
    "                           value_name=\"value\")\n",
    "\n",
    "fig = px.box(plot_data,\n",
    "             x=\"name\",\n",
    "             y=\"value\", \n",
    "             color=\"TARGET\",\n",
    "             notched=True)\n",
    "\n",
    "fig.update_layout(template= \"simple_white\", \n",
    "                  title=\"Target vs continuous variable box plot\")\n",
    "\n",
    "fig.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d4b68d",
   "metadata": {},
   "source": [
    "The chart above suggests there are some outliers in the data, most notably for **'total income ammount'**, these will be either imputed or removed from the data. The features identify a few key relationships;\n",
    "\n",
    "* **AMT_INCOME_TOTAL** - People will lower income are more likely to default;\n",
    "* **AMT_GOODS_PRICE** - For consumer loans, the higher the price of goods for which the loan is given, the less likely they are to default;\n",
    "* **AMT_ANNUITY** - Higher loan annuitys tend to see lower default rates. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e91e75",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove outliers in **AMT_INCOME_TOTAL** \n",
    "credit_risk = remove_outliers(data = credit_risk, column = 'AMT_INCOME_TOTAL')\n",
    "credit_risk = remove_outliers(data = credit_risk, column = 'AMT_INCOME_TOTAL')\n",
    "\n",
    "# remove null values \n",
    "credit_risk = credit_risk.dropna() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787ec378",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plot_data = credit_risk[['TARGET', *continuous_names]]\n",
    "plot_data = plot_data.melt(id_vars=\"TARGET\", \n",
    "                           var_name=\"name\", \n",
    "                           value_name=\"value\")\n",
    "\n",
    "fig = px.scatter(data_frame = plot_data,\n",
    "              x = 'TARGET',\n",
    "              y = 'value',\n",
    "              color = 'name', \n",
    "              color_discrete_sequence= color_palette[1:10], \n",
    "              facet_col=\"name\", \n",
    "              facet_col_wrap=2\n",
    "             )\n",
    "\n",
    "fig.update_yaxes(matches=None)\n",
    "fig.update_layout(template= \"simple_white\", \n",
    "                  title=\"Target vs continuous variables\",\n",
    "                  xaxis = dict(\n",
    "                      tickmode = 'array',\n",
    "                      tickvals = [0,1],\n",
    "                      ticktext = ['Zero', 'One']\n",
    "                  )\n",
    "                 )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80646f68",
   "metadata": {},
   "source": [
    "## Target variable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d18624",
   "metadata": {},
   "source": [
    "The target variable contains information on consumer defaults, where 1 represents a default and 0 otherwise.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e44b14d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate default rates as a % of the data \n",
    "plot_data = credit_risk['TARGET'].value_counts().reset_index() \n",
    "ratio = plot_data['TARGET'].iloc[1]/(plot_data['TARGET'].iloc[0] + plot_data['TARGET'].iloc[0])*100\n",
    "\n",
    "plot_data['index'] = plot_data['index'].astype(str)\n",
    "\n",
    "print(\"Defaults represent \" + str(round(ratio,2)) + \" % of the data\" )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "845108f8",
   "metadata": {},
   "source": [
    "The dataset is imbalanced, meaning the target class has an uneven distribution of observations, i.e. `TARGET = 1` occurs less frequently than `TARGET = 0`. Imbalanced classification is primarily challenging due to the severely skewed class distribution. This may cause poor performance in machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a74520",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot count of default rates vs non-default rates\n",
    "\n",
    "fig = px.bar(plot_data, \n",
    "             x='TARGET',\n",
    "             y = 'index', \n",
    "             color_discrete_sequence=color_palette, \n",
    "             barmode=\"stack\")\n",
    "fig.update_layout(template= \"simple_white\", \n",
    "                  title=\"Count of default rates, default = 1\", \n",
    "                  yaxis_title = \"Default\", \n",
    "                  xaxis_title = \"Count\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1480c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation of features with target variable\n",
    "corr_with_target = credit_risk[colnames + ['TARGET']].corr().iloc[:-1, -1].sort_values(ascending=False)\n",
    "\n",
    "# plot feature correlation vs target\n",
    "fig = px.bar(x=corr_with_target.values, \n",
    "             y=corr_with_target.index, \n",
    "             color_discrete_sequence= color_palette)\n",
    "\n",
    "fig.update_layout(template= \"simple_white\", \n",
    "                  title=\"Correlation with target variable\", \n",
    "                  yaxis_title = \"\", \n",
    "                  xaxis_title = \"Correlation\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d546deaf",
   "metadata": {},
   "source": [
    "## Categorical features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95070605",
   "metadata": {},
   "source": [
    "There are 10 categorical features in the data - its important to understand their relationship with the **target** variable;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ce4606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute correlation matrix of feature set \n",
    "corr = credit_risk[[*categorical_names]].corr()\n",
    "\n",
    "sns.heatmap(corr, \n",
    "            xticklabels=corr.columns.values,\n",
    "            yticklabels=corr.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec02f25",
   "metadata": {},
   "source": [
    "The categorical predictive features appear relatively uncorrelated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea83cafe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot target vs income type\n",
    "make_bar_plot(credit_risk, 'NAME_INCOME_TYPE', title= 'Income sources vs target', xaxis='', yaxis='Count (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fcb918",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target vs education type\n",
    "make_bar_plot(credit_risk, 'NAME_EDUCATION_TYPE', title= 'Education type vs target', xaxis='', yaxis='Count (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90de0b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target vs occupation type\n",
    "make_bar_plot(credit_risk, 'OCCUPATION_TYPE', title= 'Occupation type vs target', xaxis='', yaxis='Count (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2e5c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target vs occupation type\n",
    "make_bar_plot(credit_risk, 'CNT_CHILDREN', title= 'Occupation type vs target', xaxis='', yaxis='Count (%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65a0f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot target vs occupation type\n",
    "make_bar_plot(credit_risk, 'CODE_GENDER', title= 'Occupation type vs target', xaxis='', yaxis='Count (%)')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b8cc25e",
   "metadata": {},
   "source": [
    "# Transform and encode data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a604a17",
   "metadata": {},
   "source": [
    "Before applying machine learning methods, catergorical variables needs to be encoded into numerial form. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23a46cf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "names_to_encode = credit_risk.select_dtypes(include=[object], exclude=[float]).columns # create list of object names\n",
    "credit_risk[names_to_encode] = MultiColumnLabelEncoder(columns = names_to_encode).fit_transform(credit_risk[names_to_encode].astype(str))\n",
    "credit_risk.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9325874",
   "metadata": {},
   "source": [
    "Apply box-cox transformer to continuous features so that the resulting variable looks more normally distributed. This will help reduce skew in the raw variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f63f411",
   "metadata": {},
   "outputs": [],
   "source": [
    "power = PowerTransformer(method='box-cox', standardize=True)\n",
    "credit_risk[continuous_names] = power.fit_transform(credit_risk[continuous_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d68f66e",
   "metadata": {},
   "source": [
    "# Test and training datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aad0c381",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_risk['TARGET'] = credit_risk['TARGET'].astype(str)\n",
    "X = credit_risk[colnames] # X value contains all the variables except labels\n",
    "y = credit_risk['TARGET'] # these are the labe'\n",
    "\n",
    "# create training test split data sets, with test size of 30% of the data \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
    "credit_train = pd.DataFrame(X_train).assign(TARGET = y_train)\n",
    "credit_test = pd.DataFrame(X_test).assign(TARGET = y_test)\n",
    "credit_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dff2ef0",
   "metadata": {},
   "source": [
    "# Benchmark logistic model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37eb8db7",
   "metadata": {},
   "source": [
    "First a logistic model is applied to the data to act as a baseline for performance evaluation. Logistic models are used to predict the probability that an observation falls into one of two categories; `TARGET = 1` or `TARGET = 0` based on the set of predictors (features). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a20978",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up and fit logistic regression model \n",
    "logit= LogisticRegression()\n",
    "logit.fit(X_train, y_train)\n",
    "\n",
    "# Predicting the model\n",
    "pred_logit= logit.predict(X_test)\n",
    "\n",
    "\n",
    "print(\"The accuracy of logit model is:\", accuracy_score(y_test, pred_logit))\n",
    "print(classification_report(y_test, pred_logit))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a6aad25",
   "metadata": {},
   "source": [
    "Note the accuracy is 91%; however this is not a good measure due to the class imbalances in the data. The recall and precision score is 0% for `TARGET = 1`, i.e. the model is very bad at predicting true defaults, whilst the model is also misclassifying non-defaults as defaults."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08918e36",
   "metadata": {},
   "source": [
    "# Model fitting on raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c8af60",
   "metadata": {},
   "source": [
    "In this section an array of classifiers are explored. Each model is fitted to the data using a ‘training’ sample. The ‘testing’ sample is used to evaluate each model’s performance in predicting credit defaults. The model fits can be seen in the Annex. Once the models have been fitted using the training data, each model’s performance can be evaluated when applied to unseen data (out-of-sample).The algorithms include logistic, K-nearest neighbours, random forest, decision trees, ada boost, naive Bayes and gradient boosted classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b8ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = setup(\n",
    "    fold=10, # 10 fold cross validation \n",
    "    data = credit_train, # training data \n",
    "    silent = True, \n",
    "    test_data = credit_test, # test data \n",
    "    target = 'TARGET', \n",
    "    session_id = 123)\n",
    "\n",
    "# fit set of models based on the above configuration, sorted by F1 score\n",
    "best = compare_models(include = ['lr', 'knn', 'rf', 'dt', 'ada', 'nb', 'gbc'], \n",
    "                      sort = 'F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac901fa7",
   "metadata": {},
   "source": [
    "The predictive performance on the imbalanced data is poor, with the naive bayes model outperforming with an F1 score of just 17%. The accuracy statistics are misleading "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a690e740",
   "metadata": {},
   "source": [
    "The decision tree classifier performs best in terms of F1 score and recall i.e. the model is best at detecting defaults, whilst the random forest generated the best precision or best at detecting defaults out of all identified cases. That said, the model performance is poor and is not identifying defaults particuarly well.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a22216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create decision tree model, using 5-fold cross validation\n",
    "dt_fit = create_model('dt', fold=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de4b904",
   "metadata": {},
   "source": [
    "The following confusion matrix and AUC charts show the poor predictive performance of the decision tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67e98e3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot confusion matrix \n",
    "plot_model(dt_fit, plot='confusion_matrix',  plot_kwargs = {'percent' : True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373fe674",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot AUC \n",
    "plot_model(dt_fit, plot='auc')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52f1960",
   "metadata": {},
   "source": [
    "The following helps identify which features are the most import in predicting default rates under the decision tree model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322e022e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(dt_fit, 'feature')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8d11c1",
   "metadata": {},
   "source": [
    "The above chart shows that birthday, employement history, annuity, income and credit are all very important in predicting consumer default rates. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6910ca50",
   "metadata": {},
   "source": [
    "# Resampling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30a9b1c6",
   "metadata": {},
   "source": [
    "The credit risk dataset is heavility imbalanced with a small percentage of observations in the data representing defaults. Re-sampling methods are explored here. Oversampling is used, which help ensure no information from the original training set is lost and all members from the minority and majority classes are retained. However, its computationally coslty as it increases the size of the training set.\n",
    "\n",
    "Synthetic Minority Oversampling TEchnique (SMOTE) is used here as an oversampling method. SMOTE works by selecting examples that are close in the feature space, drawing a line between the examples in the feature space and drawing a new sample at a point along that line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f9cc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample using smote to oversample the minority class.\n",
    "# this synthesizes new samples from the existing ones.\n",
    "\n",
    "oversample = SMOTE()\n",
    "\n",
    "# define new training and test data using resampled data \n",
    "X_train_bal, y_train_bal = oversample.fit_resample(X_train, y_train)\n",
    "credit_train_bal = pd.DataFrame(X_train_bal).assign(TARGET = y_train_bal)\n",
    "credit_test = pd.DataFrame(X_test).assign(TARGET = y_test)\n",
    "\n",
    "# calculate default rates as a % of the data in reblanced data \n",
    "plot_data = credit_train_bal['TARGET'].value_counts().reset_index() \n",
    "ratio = plot_data['TARGET'].iloc[1]/(plot_data['TARGET'].iloc[0] + plot_data['TARGET'].iloc[1])*100\n",
    "print(\"In the reblanced data - defaults represent \" + str(round(ratio,2)) + \" % of the data\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c922152d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the rebalanced data, can now apply the same set of algorithms \n",
    "s = setup(\n",
    "    fold=10, # 10 fold cross validation \n",
    "    data = credit_train_bal, \n",
    "    silent = True, \n",
    "    test_data = credit_test, \n",
    "    target = 'TARGET', \n",
    "    session_id = 123)\n",
    "\n",
    "best = compare_models(include = ['lr', 'knn', 'rf', 'dt', 'ada', 'nb', 'gbc'], \n",
    "                      sort = 'F1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a72335",
   "metadata": {},
   "source": [
    "Rebalancing the data makes a significant improvement to the model results, the F1 score is now 87% for the random forest. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337fb21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_fit = create_model('rf', fold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f67f8f",
   "metadata": {},
   "source": [
    "# Feature engineering "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6d452",
   "metadata": {},
   "source": [
    "In this section, feature engineering is considered in an attempt to improve the model's predictive performance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f00ea6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_risk['CREDIT_INCOME_RATIO'] = credit_risk['AMT_CREDIT']/credit_risk['AMT_INCOME_TOTAL']\n",
    "credit_risk['GOODS_CREDIT_RATIO'] = credit_risk['AMT_GOODS_PRICE']/credit_risk['AMT_CREDIT']\n",
    "credit_risk['GOODS_INCOME_RATIO'] = credit_risk['AMT_GOODS_PRICE']/credit_risk['AMT_INCOME_TOTAL']\n",
    "\n",
    "# feature primatives to engineer additional model features \n",
    "default_agg_primitives =  [\"sum\", \"std\", \"max\", \"skew\", \"min\", \"mean\"]\n",
    "\n",
    "# Make an entityset and add the data \n",
    "es = ft.EntitySet(id = 'credit_train')\n",
    "\n",
    "es.entity_from_dataframe(entity_id = 'data', \n",
    "                         dataframe = credit_risk, \n",
    "                         make_index = True, \n",
    "                         index = 'index')\n",
    "\n",
    "# Run deep feature synthesis with transformation primitives\n",
    "feature_matrix, feature_defs = ft.dfs(entityset = es, \n",
    "                                      target_entity = 'data',\n",
    "                                      agg_primitives=default_agg_primitives, # use default primatives definte above\n",
    "                                      trans_primitives = ['add_numeric', 'multiply_numeric'])\n",
    "\n",
    "print('%d total Features have been created' % len(feature_defs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b9d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "credit_features = pd.DataFrame(feature_matrix)\n",
    "\n",
    "X = credit_features.drop('TARGET',axis=1) # X value contains all the variables except labels\n",
    "y = credit_features['TARGET'] # these are the labe'\n",
    "\n",
    "# create training test split data sets, with test size of 30% of the data \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.3)\n",
    "credit_features_train = pd.DataFrame(X_train).assign(TARGET = y_train)\n",
    "credit_features_test = pd.DataFrame(X_test).assign(TARGET = y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35070d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "s = setup(\n",
    "    fold=5, # 10 fold cross validation \n",
    "    data = credit_features_train, \n",
    "    silent = True, \n",
    "    test_data = credit_features_test, \n",
    "    target = 'TARGET', \n",
    "    session_id = 123)\n",
    "\n",
    "best = compare_models(include = ['lr', 'knn', 'rf', 'dt', 'ada', 'nb', 'gbc'], \n",
    "                      sort = 'F1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca689e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_fit_features = create_model('nb', cross_validation=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ec1ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(nb_fit_features, plot = 'auc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97921051",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(nb_fit_features,  plot = 'confusion_matrix', plot_kwargs = {'percent' : True})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bb8131",
   "metadata": {},
   "source": [
    "The feature selection methods improve the model performance somewhat. The F1 statistic under the naive Bayes classifier is now 18.5%, with the recall notably improved at 81%. The model still predicting a large number of false positives and false negatives. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e449504",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(nb_fit_features, plot=\"feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce6fb8bc",
   "metadata": {},
   "source": [
    "# Model hyperparamter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03b6993",
   "metadata": {},
   "source": [
    "# Bias-variance trade off"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "487e523c",
   "metadata": {},
   "source": [
    "When it comes to machine learning, there is a trade of between model performance and cost. Basic models with few degrees of freedom (logistic case) are often simple to calculate (low cost); however may lead to poorer model fits and performance (e.g. under-fitting, when there is a non-linear relationship). On the other hand, sophisticated models such as ANN can provide more accurate fits, as demonstrated above but are computationally intensive (cv ANN > 10mins to compute). In addition, complex models with a large number of parameters can lead to overfitting or be subject to a lot of variance (bias-variance trade off). The cross validation process used in this analysis can help calibrate a model’s fit, which in turn can improve predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0401c80e",
   "metadata": {},
   "source": [
    "# Overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeb6d47",
   "metadata": {},
   "source": [
    "# Scalability"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6f46770",
   "metadata": {},
   "source": [
    "Thus far, only the accuracy of the model has been considered. The scalability of the machine learning algorithm is of importance when implementing such solutions. For example, algorithm such as neural networks are powerful but computationally intensive and slow to train. The credit default data consists of ~112,000 observations, which is a relatively modest size for dataset. It is possible run these algorithms on a local machine; however given the ever growing set of records, one day this may become a problem. Solutions such as pySpark and sparklyr can help overcome such challenges along with other distributed processing systems used for big data workloads. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f81902",
   "metadata": {},
   "source": [
    "# Extensions "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ba82cd",
   "metadata": {},
   "source": [
    "The models explored in this exercise peform relatively in predicting consumer credit defaults when accounting for preferences between Type I and Type II erros. Model performance could be further enhanced via the application of some of the following techniques;\n",
    "\n",
    "1. **Regularisation**: This is a smoothing method that introduces a penalty term to the error function acting on non-zero, especially large, parameter values. Its effect is the reduction of the complexity, or the smoothening, of a model. This is useful when faced with an over-fitting model but also for feature selection, which can sometimes be problematic in neural networks.\n",
    "\n",
    "2. **Bagging**: A popular method for improving an estimator’s properties. The idea is that, when aggregating (averaging) the outputs of many estimators, one reduces the variance of the full model. When over-fitting is an issue, as for tree models, bagging is likely to improve performance.\n",
    "\n",
    "3. **Boosting**: Describes a set of methods which transform, or boost, a weak learner into a strong one (model). This means that one iteratively applies models to the data which individually have weak generalisation properties, but the final ensemble of models generalises well. \n",
    "\n",
    "4. **Simulated data**: This involves creating synthetic data of the minority class (TARGET = 1) to help rebalance the minority class. This was explore and improved model performance, but there are further re-sampling methods that could improve the predictive accuracy even further. The result could be better out of sample predictive performance as the model has ‘seen’ more instances of what a TARGET looks like and is therefore more likely to be able to identify future instances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc9e7d6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
